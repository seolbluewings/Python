{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 토픽의 개수를 미리 결정(T=4) 하고\n",
    "- 문서(documents) 에 대한 데이터 부여, 문서에서부터 Parsing해서 데이터를 직접 정제하는 것이 이상적이나 이번 경우 직접 입력하는 방식\n",
    "- documents 안에는 문서별로 단어 리스트를 입력, 증권사 리포트에서 데이터를 가져옴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [[\"은행\", \"플랫폼\", \"이자\", \"비이자\", \"Valuation\", \"성장성\", \"주가\", \"대출\"], \n",
    "    [\"자동차\", \"반도체\", \"성장성\", \"자율주행\",\"당기순이익\",\"칩\"],\n",
    "    [\"CMO\", \"바이오\", \"코로나\", \"백신\", \"공정\", \"매출액\", \"성장성\", \"모더나\", \"바이러스\"],\n",
    "    [\"실적\", \"이자\", \"비은행\", \"은행\", \"대출\", \"증권\",\"비이자\"],\n",
    "    [\"바이오시밀러\", \"코로나\", \"바이러스\", \"백신\", \"수익성\", \"바이오\"],\n",
    "    [\"5G\", \"이동통신\", \"매출\", \"이익\", \"커머스\", \"지주\"],\n",
    "    [\"스마트팩토리\", \"수익률\", \"통신\", \"IPTV\"],\n",
    "    [\"GDP\", \"운용자산\", \"M&A\", \"코로나\", \"금리\", \"중앙은행\"],\n",
    "    [\"IPTV\", \"턴어라운드\", \"5G\", \"이익\", \"인터넷\", \"통신\"],\n",
    "    [\"반도체\", \"인공지능\", \"IP\", \"플랫폼\", \"칩\", \"자율주행\"],\n",
    "    [\"원자현미경\", \"공정\", \"반도체\", \"성장\", \"수익성\", \"디스플레이\"],\n",
    "    [\"로보택시\", \"모빌리티\", \"자율주행\", \"라이다\", \"자동차\"],\n",
    "    [\"임상\", \"백신\", \"식약처\", \"3상\", \"결과발표\", \"모멘텀\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $I(z_{n}^{(d)}=t)$ : document_topic_counts\n",
    "- $\\sum_{d=1}^{D}\\sum_{n=1}^{N_{d}}I(w_{n}^{(d)}=m)I(z_{n}^{(d)}=t)$ : topic_word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_topic_counts = [Counter() for _ in documents]\n",
    "\n",
    "topic_word_counts = [Counter() for _ in range(T)]\n",
    "\n",
    "topic_counts = [0 for _ in range(T)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 각 문서(d)에 존재하는 단어의 개수 $N_{d}$ 집계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_lengths = list(map(len, documents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 전체 문서의 개수 $(D)$ 집계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = len(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 데이터에 존재하는 전체 단어의 Unique 개수 $(M)$ 집계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vec = list()\n",
    "for d in range(D):\n",
    "    N_d = list(map(len, documents))[d]\n",
    "    for n in range(N_d):\n",
    "        word_vec.append(documents[d][n])\n",
    "\n",
    "unique_word_vec = set(word_vec)\n",
    "M = len(unique_word_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_word_vec = set(word_vec)\n",
    "M = len(unique_word_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gibbs Sampling Step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Gibbs Sampling 실행 과정에서 사용하게 될 conditional distribution 에 대한 코드 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ p(z_{n}^{(d)}=t \\vert z_{-n}^{(d)},\\mathbf{w},\\alpha,\\beta) \\propto \\frac{ \\beta_{m}+ \\sum_{d=1}^{D}\\sum_{n=1}^{N_{d}}I(w_{-n}^{(d)}=m)I(z_{-n}^{(d)}=t)  }{  \\sum_{m=1}^{M} \\left( \\beta_{m}+ \\sum_{d=1}^{D}\\sum_{n=1}^{N_{d}}I(w_{-n}^{(d)}=m)I(z_{-n}^{(d)}=t) \\right) } \\times \\left( \\alpha_{t}+\\sum_{n=1}^{N_{d}}I(z_{-n}^{(d)}=t) \\right) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_weight(d, word, topic):\n",
    "    \n",
    "    alpha = 1/T\n",
    "    beta  = 1/M\n",
    "    \n",
    "    return (document_topic_counts[d][topic]+alpha)*((topic_word_counts[topic][word]+beta)/(topic_counts[topic] + M * beta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_new_topic(d, word) :\n",
    "    weights = np.zeros(T)\n",
    "    for topic in range(T) :\n",
    "        weights[topic] =topic_weight(d, word, topic)\n",
    "    total = weights.sum()\n",
    "    t = np.argmax(np.random.multinomial(1, pvals = weights/total))\n",
    "    return t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 문서에 존재하는 각 단어들에 대해 토픽 할당\n",
    "- Gibbs Sampling 수행을 위해 각 단어에 대해 토픽 최초값을 임의로 할당"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "document_topics = [[random.randrange(T) for word in document] for document in documents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- zip은 두 개의 list의 값을 병렬 추출\n",
    "- 두 개의 list가 있을 때 각 인덱스에 있는 값들을 뽑아주는 것이 zip\n",
    "- 문서에 대해 for문을 돌리면서 개수를 Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in range(D):\n",
    "    for word, topic in zip(documents[d], document_topics[d]):\n",
    "        document_topic_counts[d][topic] = document_topic_counts[d][topic] + 1 \n",
    "        topic_word_counts[topic][word] = topic_word_counts[topic][word] + 1\n",
    "        topic_counts[topic] = topic_counts[topic] + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- enumerate & zip을 통해서 iteration 별로 documents 별 개별 단어와 그에 대응하는 토픽 할당 정보를 가져옴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1000): # repetition\n",
    "    for d in range(D): # each documnet\n",
    "        for i, (word, topic) in enumerate(zip(documents[d],document_topics[d])):\n",
    "            \n",
    "            ### 각 iteration에서 선택되는 단어에 대한 토픽 정보를 제거한 상태에서 나머지 데이터를 가지고 Posterior 계산 \n",
    "            \n",
    "            document_topic_counts[d][topic] = document_topic_counts[d][topic] - 1 # 문서 별 토픽 갯수\n",
    "            topic_word_counts[topic][word] = topic_word_counts[topic][word] - 1 # 토픽 별 단어 갯수\n",
    "            topic_counts[topic] = topic_counts[topic] - 1 # 토픽 별 횟수 (각 토픽으로 할당된 단어가 몇개인가?)\n",
    "            document_lengths[d] = document_lengths[d] - 1 # 문서별 단어갯수\n",
    "            \n",
    "            # choose a new topic based on the weights\n",
    "            new_topic = choose_new_topic(d, word) # iteration 에 해당하는 단어에 대한 토픽 할당 \n",
    "            document_topics[d][i] = new_topic # 새로 할당된 토픽 정보로 데이터 업데이트 처리\n",
    "            \n",
    "            # and now add it back to the counts\n",
    "            document_topic_counts[d][new_topic] = document_topic_counts[d][new_topic] + 1 # 문서별 토픽 갯수\n",
    "            topic_word_counts[new_topic][word] =  topic_word_counts[new_topic][word] + 1 # 토픽 별 단어 갯수\n",
    "            topic_counts[new_topic] = topic_counts[new_topic] + 1 # 토픽 별 횟수 (각 토픽으로 할당된 단어가 몇개인가?)\n",
    "            document_lengths[d] = document_lengths[d] + 1 # 문서별 단어갯수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 토픽 별로 상위 6개 단어에 대한 분포를 확인\n",
    "- [이자, 비이자, 대출, 은행] 과 같은 단어가 하나의 토픽으로 묶여 Topic 1은 금융회사 관련된 문서로 추정\n",
    "- [코로나, 백신, 바이러스, 공정, 바이오] 단어가 하나의 토픽으로 묶여 Topic 2는 바이오 관련 문서로 추정\n",
    "- [5G, 통신 ,IPTV] 등과 같은 단어가 하나의 토픽으로 묶인 결과를 보아 Topic 3는 통신사 관련된 문서로 추정\n",
    "- [반도체, 자율주행, 칩, 자동차] 단어가 하나의 토픽으로 묶여 Topic 4은 자율주행을 위한 차량용 반도체 관련된 문서로 추정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['Topic1','Topic2','Topic3','Topic4'], index=['Top'+str(i) for i in range(1,6)])\n",
    "\n",
    "for k, word_counts in enumerate(topic_word_counts):\n",
    "    for i, (word, count) in enumerate(word_counts.most_common(6)): # 각 토픽별로 top 10 단어\n",
    "            df.loc['Top'+str(i+1),'Topic'+str(k+1)] = word+'({})'.format(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic1</th>\n",
       "      <th>Topic2</th>\n",
       "      <th>Topic3</th>\n",
       "      <th>Topic4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Top1</th>\n",
       "      <td>은행(2)</td>\n",
       "      <td>코로나(3)</td>\n",
       "      <td>이익(2)</td>\n",
       "      <td>자율주행(3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Top2</th>\n",
       "      <td>이자(2)</td>\n",
       "      <td>백신(3)</td>\n",
       "      <td>IPTV(2)</td>\n",
       "      <td>반도체(3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Top3</th>\n",
       "      <td>통신(2)</td>\n",
       "      <td>수익성(2)</td>\n",
       "      <td>5G(2)</td>\n",
       "      <td>자동차(2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Top4</th>\n",
       "      <td>비이자(2)</td>\n",
       "      <td>공정(2)</td>\n",
       "      <td>플랫폼(2)</td>\n",
       "      <td>칩(2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Top5</th>\n",
       "      <td>대출(2)</td>\n",
       "      <td>바이오(2)</td>\n",
       "      <td>증권(1)</td>\n",
       "      <td>당기순이익(1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Top6</th>\n",
       "      <td>성장성(2)</td>\n",
       "      <td>바이러스(2)</td>\n",
       "      <td>커머스(1)</td>\n",
       "      <td>스마트팩토리(1)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Topic1   Topic2   Topic3     Topic4\n",
       "Top1   은행(2)   코로나(3)    이익(2)    자율주행(3)\n",
       "Top2   이자(2)    백신(3)  IPTV(2)     반도체(3)\n",
       "Top3   통신(2)   수익성(2)    5G(2)     자동차(2)\n",
       "Top4  비이자(2)    공정(2)   플랫폼(2)       칩(2)\n",
       "Top5   대출(2)   바이오(2)    증권(1)   당기순이익(1)\n",
       "Top6  성장성(2)  바이러스(2)   커머스(1)  스마트팩토리(1)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
